{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f16c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "folderpath =r\"C:\\Users \\Dell \\OneDrive \\Documents \\wikii\" \n",
    "filepaths= [os.path.join (folderpath, name) for name in os.listdir(folderpath) ] \n",
    "all_files = [ ]\n",
    " for path in filepaths : \n",
    "        with open (path, 'r encoding=\"IS0- 8859-1\") as f:\n",
    "file = f.readlines () \n",
    "all_files. append (file)\n",
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e75e161",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeOfmyfiles =len (all_files) \n",
    "print (size0fmyfiles)\n",
    "\n",
    "listToStr =' '.join (map (str, all_files))\n",
    "listToStr\n",
    "#%pip install nl tk\n",
    "#import nltk \n",
    "from nltk.corpus import stopwords #nltk.download ( 'stopwords') \n",
    "from nltk.tokenize import word_tokenize text= word_tokenize (listToStr) \n",
    "data = [word for word in text if not word in stopwords . words ()]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f2388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk. stem import WordNetLemmatizer \n",
    "lemma= WordNet Lemmatizer () \n",
    "lem=[ ]\n",
    "for r in data:\n",
    "       lem. append (lemma.lemmatize (r))\n",
    "print (lemma. lemmatize( 'bats '))\n",
    "\n",
    "import re \n",
    "dataupdate= [ ] \n",
    "dataupdate=[re. sub('[^a-zA-z0-9]+ ', '', _) for _ in lem] \n",
    "dataupdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8a8c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction. text import TfidfVectorizer \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn. metrics import adjusted_rand_score\n",
    " # import warnings filter \n",
    "from warnings import simplefilter\n",
    " #ignore all future warnings\n",
    "simplefilter (action='ignore\", category=FutureWarning)\n",
    "vectorizer TfidfVectorizer(stop_words= 'english')\n",
    "X = vectorizer. fit_transform(dataupdate)\n",
    "true_k = 5 \n",
    "model = KMeans (n_clusters=true_k, init= 'k-means++ ', max_iter=100, n_init=1) model.fit (X)\n",
    "print (\"Top terms per cluster: \") order_centroids = model . cluster_centers_. argsort () [:, ::-1] \n",
    "terms = vectorizer.get_feature_names () for i in range(true k): \n",
    "         print(\"Cluster %d:\" % i), \n",
    "         for ind in order_centroids[i, :10]:\n",
    "               print( ' %s' % terms[ind]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b2626",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Guessing :\")\n",
    "Y = vectorizer. transform ( [\"restraint\"]) guessing= model.predict (Y)\n",
    "print (guessing)\n",
    "Y =vectorizer. transform ( [\"claim that this cetral aspect of anarchism is definitive is to sell anarchism short\"])\n",
    " guessing = model.predict (Y) \n",
    "print (guessing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
